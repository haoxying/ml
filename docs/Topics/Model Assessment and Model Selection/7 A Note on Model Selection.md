It is important to note that we are always assuming that no model selection procedure is performed using the training data, i.e. no feature selection, hyperparameter tuning nor selection of an imputation procedure. Otherwise, there would be correlations between the selected model, which is a random variable itself, and any statistics used in inference later, e.g. confidence intervals, hypothesis tests or error estimates. Consider a regression problem in which the data is used to do feature selection. Intuitively, in a significance test for the chosen variables, these are necessarily going to be significant for the data that was used to select them in the first place. Additionally, it is well known that the CV estimate of the error _of the model chosen by a selection procedure_ is strongly biased downwards, because the statistics of the extreme value $\min\{\widehat{Err}_{1},…,\widehat{Err}_{k}\}$ are different from those of the $\widehat{Err}_{i}$. This can be problematic if the bias is significant and we need to rely on it to make decisions. For example, suppose that we have two different methods A and B. A has one tuning parameter while B has five. Which one should we use? If the best model we trained using A has validation error 0.1 and the best model we trained using B has validation error 0.06, should we pick B over A? Not necessarily. The expected test error of method B associated with its model picking rule can still be higher than that of the method A.

The simplest approach for simultaneous model selection and error estimation is a simple train - test split of the data. But this comes at cost of both bias and variance because of the reduced data on which the models are chosen and the winner finally evaluated. 

If computational resources allow for it, it is common to use a nested CV procedure, which has great computational cost, and for which there are no known theoretical guarantees.
![[Pasted image 20230131052132.png]]

Note the nested CV in 3 (it can be used to estimate the "variance of the CV error") is slightly different from the nested CV in 1 (it can be used to estimate the prediction error of the selected model). 

References: 
1. Bias in error estimation when using cross-validation for model selection, Sudhir Varma, Richard Simon. BMC Bioinformatics (2006)
2. Test Error Estimation after Model Selection Using Validation Error, Leying Guan. arXiv:1801.02817 (2018)
3. Cross-validation: what does it estimate and how well does it do it?, Stephen Bates, Trevor Hastie, Robert Tibshirani. (2021)
4. More generally on this topic: Post-Model-Selection Inference