{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Syllabus","text":""},{"location":"#textbook","title":"Textbook","text":"<p>The Elements of Statistical Learning</p> <p>An Introduction to Statistical Learning </p>"},{"location":"#reference","title":"Reference","text":"<p>Hastie\u2019s homepage: https://hastie.su.domains/index.html </p> <p>Friedman\u2019s homepage: https://jerryfriedman.su.domains </p> <p>Python code for ISL: https://github.com/JWarmenhoven/ISLR-python </p>"},{"location":"#books","title":"Books","text":"<p>Computer-Age Statistical Inference\u00a0</p> <p>Pattern Recognition and Machine Learning (Bible)\u00a0</p> <p>Probabilistic Machine Learning: https://probml.github.io/pml-book/book1.html </p> <p>PML: Advanced Topics: https://probml.github.io/pml-book/book2.html </p>"},{"location":"#open-courseswith-video","title":"Open Courses(with video):","text":"<ol> <li> <p>Stanford ISL(Hastie):\u00a0https://www.edx.org/course/statistical-learning </p> </li> <li> <p>NTU Machine Learning Foundation and Technics(\u6797\u8f69\u7530)\u00a0https://www.coursera.org/instructor/htlin </p> </li> <li> <p>BIMSA: Sequential Model and Time series\u00a0https://www.bimsa.cn/newsinfo/749652.html </p> </li> <li> <p>BIMSA: Bayesian Machine Learning\u00a0https://www.bimsa.cn/newsinfo/752150.html </p> </li> <li> <p>Bilibili: \u673a\u5668\u5b66\u4e60\u767d\u677f\u63a8\u5bfc\u00a0https://www.bilibili.com/video/BV1aE411o7qd </p> </li> </ol>"},{"location":"#requirement","title":"\u6b63\u5f0f\u7ec4 requirement","text":"<p>Lecture: 120-180 mins (1. content 2. slides(72h before 8pm Sat))\u00a0</p> <p>Assignment: problems in textbooks, proof in textbook and some papers, and code implementation\u00a0</p> <p>Tutorial: lecture (continued), assignments, interview questions\u00a0</p>"},{"location":"#schedule","title":"Schedule","text":"<p>Week 1(10.29): Orientation\u00a0</p>"},{"location":"#basic-topics","title":"Basic Topics","text":"<p>Week 2(11.5): Introduction1-Linear Algebra (Xiaonan Peng)\u00a0</p> <p>Week 3(11.12): Introduction2-Optimization (Xiaonan Peng)\u00a0</p> <p>Week 4(11.19): Lecture 1: Linear Regression (Xiaonan Peng)\u00a0</p> <p>ISL: 3.1-3.5; ESL: 3.1-3.2\u00a0</p> <p>Week 5(11.26): Lecture 2: Linear Classification (Zhongcan Wang)\u00a0</p> <p>ISL: 4.1-4.5; ESL: 4.1-4.2, 4.3.1-4.3.2, 4.4\u00a0</p> <p>Week 6(12.3): Lecture 1(continued): Regularization (Xiaonan Peng)\u00a0</p> <p>ISL: 6.1-6.2; ESL: 3.3-3.4\u00a0</p> <p>Week 7(12.10): Tutorial 1(Xiaonan Peng)\u00a0</p> <p>Week 8(12.17): Tutorial 2(Zhongcan Wang)\u00a0</p> <p>Week 9 &amp; 10: Break\u00a0</p> <p>Week 11(1.7): Lecture 3: Resampling, model selection: CV and Bootstrap (Hao Ying)\u00a0</p> <p>ISL: 5.1-5.2; ESL: 7.1-7.12, 8.1-8.4\u00a0</p> <p>Week 12(1.14): Lecture 4: Additive Model and Kernel Smoothing Method (Bowei Fan) .14</p> <p>ISL: 7.1-7.7; ESL: 5.1-5.6, 6.1-6.9\u00a0</p> <p>Week 13: Break</p> <p>Week 14(1.28): Interview question of regression(Xiaonan Peng)</p> <p>Week 15(2.4): Tutorial 3(Hao Ying) </p> <p>Week 16(2.18): Tutorial 4(Bowei Fan)</p> <p>Week 17(2.25): Lecture 5: Tree and Random Forest (Zhuangzhuang Han)\u00a0</p> <p>ISL: 8.1, 8.2.1-8.2.2; ESL: 9.1-9.7, 15.1-15.4\u00a0\u00a0</p> <p>Week 18(3.4): Lecture 6: Adaboost (Zhe Yang) </p> <p>ISL: 8.2.3-8.2.5; ESL:10.1-10.14, 16.1-16.3\u00a0</p> <p>Week 19(3.11): Break</p> <p>Week 20(3.18): Tutorial 5 (Zhuangzhuang Han &amp; Hao Ying)\u00a0</p> <p>Week 21(3.25): Lecture 7: Deep Learning (Jade Tan) </p> <p>Week 22(4.1):  Lecture 6(Continued): Gradient Boost (Yang Zhe)</p> <p>Week 23(4.8): Break</p> <p>Week 24(4.15): Lecture 8: SVM (Xiaonan Peng) </p> <p>Week 25(4.22): Lectuer 7(Continued): AlexNet (Jade Tan)\u00a0</p>"},{"location":"#advanced-topics-advanced-ml","title":"Advanced Topics\u00a0(Advanced ML)","text":"<p>Lecture 9: Prototype Analysis ,KNN, and Survival Analysis</p> <p>ISL: 11.1-11.7 ESL: 13.1-13.5\u00a0</p> <p>Lecture 10: Unsupervised Learning, matrix factorization</p> <p>Lecture 11: Sparse Modeling and Lasso </p> <p>CASI: 16.1-16.8\u00a0\u00a0</p> <p>Lecture 12: EM Algorithm and GMM</p> <p>Lecture 13: Variational Inference</p> <p>Lecture 14: Sequential Data: HMM </p> <p>Lecture 15: Generative Model: VAE</p> <p>Lecture 16: GAN</p> <p>Lecture 17: Flow-based method</p> <p>Lecture 16: Diffusion Model </p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/1%20Outline/","title":"1 Outline","text":""},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/2%20Bias%20and%20Variance/","title":"2 Bias and Variance","text":"<ul> <li>Test Error (Generalization Error): \\(Err_\\mathcal{T}=E[L(Y,\\hat{f}(X))|\\mathcal{T}]\\) (light red)</li> <li>Prediction Error: \\(Err = E[L(Y, \\hat{f}(X))] = E[Err_\\mathcal{T}]\\) (solid red)</li> <li>Training Error: \\(\\overline{err}=\\frac{1}{N} \\sum_{i=1}^{N} L(y_{i}, \\hat{f}(x_{i}))\\) (light blue)</li> <li>Expected Training Error: \\(E[\\overline{err}]\\) (solid blue) </li> </ul> <p>\u201cEverything should be made as simple as possible, but not simpler.\u201d</p> <p>\u2014 Albert Einstein</p> <p>Occam\u2019s Razor </p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/3%20Cross-Validation/","title":"3 Cross Validation","text":""},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/3%20Cross-Validation/#cross-validation-procedure","title":"Cross-Validation Procedure","text":"<p> In general, we have \\(K\\)-fold cross-validation. As \\(K\\) increases, the bias of the error estimation decreases while the variance of the error estimation increases. * We only use \\(\\frac{K-1}{K}\\) of the data to fit the model each time, so the bias is large when \\(K\\) is small. But this also depends on how much data we have. (See plot below. 5-fold cross validation, 50 observations vs 200 observations.)  * If \\(K\\) is large (leave-one-out cross-validation in the extreme case), all experiments in cross-validation are pretty much the same (high overlap), so the variance is high.</p> <p>Since we have \\(K\\) experiments, can we estimate the standard error of the error estimation? Not really. It does not give good estimate of standard error if we do it naively due to the correlation in these experiments.</p> <p></p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/3%20Cross-Validation/#right-vs-wrong","title":"Right vs Wrong","text":"<p>\u2022 Consider a simple classifier applied to some two-class data:  1. Starting with 5000 predictors and 50 samples, find the 100 predictors having the largest correlation with the class labels.  2. We then apply a classifier such as logistic regression, using only these 100 predictors.</p> <ul> <li>Wrong: Apply cross-validation in step 2. </li> <li>Right: Apply cross-validation to steps 1 and 2.</li> </ul>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/4%20Bootstrap/","title":"4 Bootstrap","text":"<p> \\(Pr\\{\\text{observation }i \\in \\text{bootstrap sample b}\\} = 1- \\left( 1- \\frac{1}{N} \\right)^{N} \\approx 1-\\frac{1}{e}=0.632\\)</p> <p>How can we apply the bootstrap to estimate prediction error? * One approach would be to fit the model in question on a set of bootstrap samples, and then keep track of how well it predicts the original training set. This however, underestimates the error due to overlapping. * For each observation, we only keep track of predictions from bootstrap samples not containing that observation. The leave-one-out bootstrap estimate of prediction error is defined by     $$ \\widehat{Err}^{(1)} = \\frac{1}{N} \\sum^{N}{i=1} \\frac{1}{|C^{-i}|} \\sum{b\\in C^{-i}} L(y_{i}, \\hat{f}^{*b}(x_{i})).$$ The average number of distinct observations in each boot- strap sample is about 0.632 * N , so its bias will roughly behave like that of twofold cross-validation. Thus if the learning curve has considerable slope at sample size N/2, the leave-one out bootstrap will be biased upward as an estimate of the true error.</p> <p>The \u201c.632 estimator\u201d is designed to alleviate this bias. It is defined by $$ \\widehat{Err}^{(.632)} =.368\\overline{err} + .632 \\widehat{Err}^{(1)}.$$</p> <p>It gets complicated... Keep it simple and use CV.</p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/5%20Estimates%20of%20In-Sample%20Prediction%20Error/","title":"5 Estimates of In Sample Prediction Error","text":"<p>Instead of extra-sample error like \\(Err_{\\mathcal{T}}\\), we consider in-sample error \\(\\(Err_{in} = \\frac{1}{N} \\sum_{i=1}^{N}E_{Y^0}[L(Y^{0}_{i} , \\hat{f}(x_{i}))|\\mathcal{T}]\\)\\) where \\(Y^{0}_{i}\\) is new response value at \\(x_{i}\\).</p> <p>Intuitively, we can see that \\(\\overline{err}\\) underestimates \\(Err_{in}\\). Homework (Ex 7.4) will ask you to show that \\(\\(E_{y}(Err_{in}) = E_{y}(\\overline{err})+\\frac{2}{N}\\sum_{i=1}^{N}Cov(\\hat{y_{i}}, y_{i})\\)\\) in some special case.</p> <p>\\(\\sum_{i=1}^{N}Cov(\\hat{y_{i}}, y_{i})=d \\sigma_{\\epsilon}^2\\) if \\(\\hat{y}_{i}\\) is obtained by a linear fit with \\(d\\) inputs.</p> <p>We can use \\(C_{p}\\) statistic to approximate the in-sample error for model selection: \\(\\(C_{p} = \\overline{err} + 2 \\frac{d}{N} \\hat{\\sigma}_{\\epsilon}^2.\\)\\)</p> <p>From information theory perspective (minimize the KL divergence between the model distribution and the true distribution), we can get similar but more general formula: \\(\\(-2E[\\log Pr_{\\hat{\\theta}}(Y)] \\approx -\\frac{2}{N}E[log lik]+2 \\frac{d}{N},\\)\\) where \\(loglik = \\sum_{i=1}^{N}\\log Pr_{\\hat{\\theta}}(y_{i})\\).</p> <p>Akaike information criterion (AIC): \\(\\(AIC = -\\frac{2}{N} loglik + 2 \\frac{d}{N}.\\)\\)</p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/6%20The%20Bayesian%20Approach/","title":"6 The Bayesian Approach","text":"<p>For a set of candidate models \\(\\mathcal{M}_{m}\\), \\(\\(Pr(\\mathcal{M}_{m}|Z) \\propto Pr(\\mathcal{M}_{m}) Pr(Z|\\mathcal{M}_{m}).\\)\\) Assuming the prior over all models are uniform, we only need to compare \\(\\(Pr(Z|\\mathcal{M}_{m}) \\propto \\int Pr(Z|\\theta_{m}, \\mathcal{M}_{m}) Pr(\\theta_{m}|\\mathcal{M}_{m})d\\theta_{m}\\)\\)</p> <p>Laplace approximation to the integral followed by some other simplifications (Ripley, 1996, page 64) to the above equation gives</p> \\[\\log Pr(Z|\\mathcal{M}_{m}) = \\log Pr(Z|\\hat{\\theta}_{m}, \\mathcal{M}_{m}) - \\frac{d_{m}}{2} \\log N + O(1).\\] <p>Bayesian information criterion (BIC): \\(\\(BIC = -2 loglik + (\\log N) d.\\)\\)</p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/7%20A%20Note%20on%20Model%20Selection/","title":"7 A Note on Model Selection","text":"<p>It is important to note that we are always assuming that no model selection procedure is performed using the training data, i.e. no feature selection, hyperparameter tuning nor selection of an imputation procedure. Otherwise, there would be correlations between the selected model, which is a random variable itself, and any statistics used in inference later, e.g. confidence intervals, hypothesis tests or error estimates.\u00a0Consider a regression problem in which the data is used to do feature selection. Intuitively, in a significance test for the chosen variables, these are necessarily going to be significant for the data that was used to select them in the first place. Additionally, it is well known that the CV estimate of the error\u00a0of the model chosen by a selection procedure\u00a0is strongly biased downwards, because the statistics of the extreme value\u00a0\\(\\min\\{\\widehat{Err}_{1},\u2026,\\widehat{Err}_{k}\\}\\) are different from those of the\u00a0\\(\\widehat{Err}_{i}\\). This can be problematic if the bias is significant and we need to rely on it to make decisions. For example, suppose that we have two different methods A and B. A has one tuning parameter while B has five. Which one should we use? If the best model we trained using A has validation error 0.1 and the best model we trained using B has validation error 0.06, should we pick B over A? Not necessarily. The expected test error of method B associated with its model picking rule can still be higher than that of the method A.</p> <p>The simplest approach for simultaneous model selection and error estimation is a simple train - test split of the data. But this comes at cost of both bias and variance because of the reduced data on which the models are chosen and the winner finally evaluated. </p> <p>If computational resources allow for it, it is common to use a nested CV procedure, which has great computational cost, and for which there are no known theoretical guarantees. </p> <p>Note the nested CV in 3 (it can be used to estimate the \"variance of the CV error\") is slightly different from the nested CV in 1 (it can be used to estimate the prediction error of the selected model). </p> <p>References:  1. Bias in error estimation when using cross-validation for model selection,\u00a0Sudhir Varma, Richard Simon.\u00a0BMC Bioinformatics\u00a0(2006) 2. Test Error Estimation after Model Selection Using Validation Error,\u00a0Leying Guan.\u00a0arXiv:1801.02817 (2018) 3. Cross-validation: what does it estimate and how well does it do it?,\u00a0Stephen Bates, Trevor Hastie, Robert Tibshirani.\u00a0(2021) 4. More generally on this topic: Post-Model-Selection Inference</p>"},{"location":"Topics/Model%20Assessment%20and%20Model%20Selection/8%20Double%20Descent/","title":"8 Double Descent","text":"<p> When the models are large enough to fit the training set, continuing increasing the model complexity can lead to a second decline of the testing error. (e.g. Error peaks when number of parameters is about equal to the sample size for linear regression.) </p> <p>Some theoretical analysis on simple models (e.g. see Two Models of Double Descent for Weak Features)</p> <p>  Since early stopping has the effect of regularization (therefore has the effect of lowering the model complexity), the double descent also appears as training time increases for large model. Be aware if you use early stopping when training a DL model.</p> <p> More data sometimes hurts if you model is not large enough.</p>"}]}